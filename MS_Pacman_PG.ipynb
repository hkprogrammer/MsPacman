{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba2z3AjwRCXL"
   },
   "source": [
    "# Ms. Pacman Agent\n",
    "## Policy Gradient with Neural Network\n",
    "References:\n",
    "- _Hugging Face policy gradient_ https://huggingface.co/learn/deep-rl-course/unit4/hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0O8vK63AdDd"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XytnVNhNd5eL"
   },
   "outputs": [],
   "source": [
    "# # Virtual display\n",
    "# # Only needed for Google Colab\n",
    "# # Code from CS 175 HW 2\n",
    "# %%capture\n",
    "# !apt install python-opengl\n",
    "# !apt install ffmpeg\n",
    "# !apt install xvfb\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install pyglet==1.5.1\n",
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3JopN98-SJN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import envs\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import imageio\n",
    "\n",
    "env = gym.make(\"ALE/MsPacman-ram-v5\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwk3fXOS-SyY"
   },
   "outputs": [],
   "source": [
    "# print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "# print(\"Observation Space\", env.observation_space)\n",
    "# print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jien_p6AwmX"
   },
   "outputs": [],
   "source": [
    "# print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "# print(\"Action Space Shape\", env.action_space.n)\n",
    "# print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAQuY62uAeLt"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group:\n",
    "#   - class inherits nn.Sequential rather than nn.Module\n",
    "#   - change to constructor method and deletion of explicitly defined forward method\n",
    "class Policy(nn.Sequential):\n",
    "  def __init__(self, n_layers, h_size, dropout, s_size, a_size):\n",
    "    layers = []\n",
    "\n",
    "    in_features = s_size\n",
    "    for i in range(n_layers):\n",
    "      layers.append(nn.Linear(in_features, h_size))\n",
    "      layers.append(nn.ReLU())\n",
    "      layers.append(nn.Dropout(0))\n",
    "      in_features = h_size\n",
    "    layers.append(nn.Linear(in_features, a_size))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    super().__init__(*layers)\n",
    "\n",
    "  def act(self, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs = self.forward(state).cpu()\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3P-qZ9Y-HLI"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group:\n",
    "#   - changes to reward for training\n",
    "#   - ensure changes to reward doesn't affect score output\n",
    "def reinforce(policy, optimizer, print_every, n_training_episodes, max_t, \n",
    "              gamma, ghost_reward, policy_file_name, step_penalty_multiplier, \n",
    "              longevity_exponential=0, dot_extra_reward=0, energy_pill_extra_reward=0):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in tqdm(range(1, n_training_episodes+1)):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state,game_env = env.reset()\n",
    "        \n",
    "        # Variables for reward changes\n",
    "        step_num = 0\n",
    "        score_adjustments = 0\n",
    "        rewards_this_life = 0\n",
    "        step_penalty = 1\n",
    "        cur_step_penalty = step_penalty\n",
    "\n",
    "        for t in range(max_t):\n",
    "            old_game_env = game_env\n",
    "\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, game_env = env.step(action)\n",
    "            \n",
    "            # Longevity reward. More reward gathered for each life, larger reward\n",
    "            if old_game_env[\"lives\"] > game_env[\"lives\"]:\n",
    "                longevity_reward = longevity_exponential ** rewards_this_life\n",
    "                rewards_this_life = 0\n",
    "                reward += longevity_reward\n",
    "                score_adjustments -= longevity_reward\n",
    "                rewards.append(reward)\n",
    "                continue\n",
    "            \n",
    "            reward_change = 0\n",
    "            \n",
    "            # Equal penalty for eating ghost\n",
    "            if reward // 100 == 2:\n",
    "              reward_change = reward - 200 + ghost_reward\n",
    "              score_adjustments += 200 - ghost_reward\n",
    "            elif reward // 100 == 4:\n",
    "              reward_change = reward - 400 + ghost_reward\n",
    "              score_adjustments += 400 - ghost_reward\n",
    "            elif reward // 100 == 8:\n",
    "              reward_change = reward - 800 + ghost_reward\n",
    "              score_adjustments += 800 - ghost_reward\n",
    "            elif reward // 100 == 16:\n",
    "              reward_change = reward - 1600 + ghost_reward\n",
    "              score_adjustments += 1600 - ghost_reward\n",
    "            \n",
    "            # Penalty for going many steps without eating dot\n",
    "            if reward % 100 == 10:\n",
    "                cur_step_penalty = step_penalty\n",
    "                reward_change += dot_extra_reward\n",
    "                score_adjustments -= dot_extra_reward\n",
    "            elif reward % 100 == 50:\n",
    "                cur_step_penalty = step_penalty\n",
    "                reward_change += energy_pill_extra_reward\n",
    "                score_adjustments -= energy_pill_extra_reward\n",
    "            else:\n",
    "                cur_step_penalty *= step_penalty_multiplier\n",
    "                reward_change -= step_penalty\n",
    "                score_adjustments += step_penalty\n",
    "            \n",
    "            rewards.append(reward + reward_change)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_sum = sum(rewards) + score_adjustments\n",
    "        scores_deque.append(reward_sum)\n",
    "        scores.append(reward_sum)\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = rewards[t] + gamma * (disc_return_t if t + 1 < n_steps else 0)\n",
    "          returns.appendleft(disc_return_t)\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}\\tStandard Deviation: {:.2f}'\n",
    "                  .format(i_episode, np.mean(scores_deque), np.std(scores_deque)))\n",
    "\n",
    "    torch.save(policy, policy_file_name)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUU61cWi0viw"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'gamma': 0.9999819193245816, \n",
    "    'n_layers': 1, \n",
    "    'h_size': 175, \n",
    "    'dropout': 0.44984866197635065, \n",
    "    'lr': 6.166629462708628e-05, \n",
    "    'longevity_exponential': 1.006491852944776, \n",
    "    'step_penalty_multiplier': 1.0386448544834312, \n",
    "    'dot_extra_reward': 13, \n",
    "    'energy_pill_extra_reward': 12,\n",
    "    \n",
    "    \"ghost_reward\": 0,\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"policy_file_name\": \"Policies/MsPacManPG_optimized.pt\",\n",
    "    \"n_training_episodes\": 10000,\n",
    "#     \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 50000,\n",
    "    \"env_id\": \"ALE/MsPacman-ram-v5\",\n",
    "    \"s_size\": 128,\n",
    "    \"a_size\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3YigzWp0viw"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "policy = Policy(hyperparameters[\"n_layers\"], hyperparameters[\"h_size\"],\n",
    "                hyperparameters[\"dropout\"], hyperparameters[\"s_size\"],\n",
    "                hyperparameters[\"a_size\"]).to(device)\n",
    "optimizer = getattr(optim, hyperparameters[\"optimizer\"])(policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFmmyptAsU6A"
   },
   "outputs": [],
   "source": [
    "# Load saved policy\n",
    "policy = torch.load(hyperparameters[\"policy_file_name\"])\n",
    "policy.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOrLawnv0viw"
   },
   "outputs": [],
   "source": [
    "# Train and save neural network\n",
    "scores = reinforce(policy, optimizer, print_every=1000, \n",
    "                   n_training_episodes=hyperparameters[\"n_training_episodes\"],\n",
    "                   max_t=hyperparameters[\"max_t\"],\n",
    "                   gamma=hyperparameters[\"gamma\"],\n",
    "                   ghost_reward=hyperparameters[\"ghost_reward\"],\n",
    "                   policy_file_name=hyperparameters[\"policy_file_name\"], \n",
    "                   step_penalty_multiplier=hyperparameters[\"step_penalty_multiplier\"],\n",
    "                   longevity_exponential=hyperparameters[\"longevity_exponential\"],\n",
    "                   dot_extra_reward=hyperparameters[\"dot_extra_reward\"],\n",
    "                   energy_pill_extra_reward=hyperparameters[\"energy_pill_extra_reward\"]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save scores to csv\n",
    "# import csv\n",
    "# with open(\"MsPacMan_training_scores.csv\", 'w') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training progress\n",
    "# plt.plot(scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcMaUOkpX1VW"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(1, n_eval_episodes + 1)):\n",
    "        state, game_env = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, _, game_env = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwvMTczpZ8G2"
   },
   "outputs": [],
   "source": [
    "mean, std = evaluate_agent(env, hyperparameters[\"max_t\"], 50, policy)\n",
    "print(f\"Mean: {mean}; Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "703fKAx4eM0Q"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory_best, out_directory_worst, episodes, fps=30):\n",
    "  \"\"\"Save rendering of best and worst performing episode out of desired number of episodes\n",
    "  \"\"\"\n",
    "  worst_score = 999999999\n",
    "  best_score = 0\n",
    "  \n",
    "  for i in tqdm(range(1, episodes + 1)):\n",
    "      \n",
    "      done = False\n",
    "      state, game_env = env.reset()\n",
    "      score = 0\n",
    "      last_life_frame = 0\n",
    "  \n",
    "      while not done:\n",
    "        old_game_env = game_env\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _ = policy.act(state)\n",
    "        new_state, reward, done, _, game_env = env.step(action)\n",
    "\n",
    "#         if old_game_env[\"lives\"] != game_env[\"lives\"]:\n",
    "#             print(\"Lived for {} frames\".format(old_game_env[\"episode_frame_number\"] - 0))\n",
    "#             last_life_frame = old_game_env[\"episode_frame_number\"]\n",
    "\n",
    "        score += reward\n",
    "      \n",
    "      if score < worst_score:\n",
    "        worst_game = env.render()\n",
    "        worst_score = score\n",
    "      elif score > best_score:\n",
    "        best_game = env.render()\n",
    "        best_score = score\n",
    "        \n",
    "#       if i % 100 == 0:  \n",
    "#         print(\"Best score is {}, as of iteration {}\".format(best_score, i))\n",
    "#         print(\"Worst score is {}, as of iteration {}\".format(worst_score, i))\n",
    "  print(\"Best score is {}\".format(best_score))\n",
    "  print(\"Worst score is {}\".format(worst_score))\n",
    "  imageio.mimsave(out_directory_best, best_game, fps=fps)\n",
    "  imageio.mimsave(out_directory_worst, worst_game, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vZcv54ceR6k"
   },
   "outputs": [],
   "source": [
    "replay_env = gym.make(\"ALE/MsPacman-ram-v5\", render_mode=\"rgb_array_list\")\n",
    "record_video(replay_env, policy, './MsPacMan_replay_best.mp4', './MsPacMan_replay_worst.mp4', 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
