{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba2z3AjwRCXL"
   },
   "source": [
    "#Ms. Pacman Agent\n",
    "##Policy Gradient with Neural Network\n",
    "References:\n",
    "- _Hugging Face policy gradient_ https://huggingface.co/learn/deep-rl-course/unit4/hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0O8vK63AdDd"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XytnVNhNd5eL"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "# Only needed for Google Colab\n",
    "# Code from CS 175 HW 2\n",
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a3JopN98-SJN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import envs\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import imageio\n",
    "\n",
    "env = gym.make(\"ALE/MsPacman-ram-v5\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwk3fXOS-SyY"
   },
   "outputs": [],
   "source": [
    "# print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "# print(\"Observation Space\", env.observation_space)\n",
    "# print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jien_p6AwmX"
   },
   "outputs": [],
   "source": [
    "# print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "# print(\"Action Space Shape\", env.action_space.n)\n",
    "# print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bAQuY62uAeLt"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group:\n",
    "#   - class inherits nn.Sequential rather than nn.Module\n",
    "#   - change to constructor method and deletion of explicitly defined forward method\n",
    "class Policy(nn.Sequential):\n",
    "  def __init__(self, n_layers, h_size, dropout, s_size, a_size):\n",
    "    layers = []\n",
    "\n",
    "    in_features = s_size\n",
    "    for i in range(n_layers):\n",
    "      layers.append(nn.Linear(in_features, h_size))\n",
    "      layers.append(nn.ReLU())\n",
    "      layers.append(nn.Dropout(0))\n",
    "      in_features = h_size\n",
    "    layers.append(nn.Linear(in_features, a_size))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    super().__init__(*layers)\n",
    "\n",
    "  def act(self, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs = self.forward(state).cpu()\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "o3P-qZ9Y-HLI"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group:\n",
    "#   - changes to reward for training\n",
    "#   - ensure changes to reward doesn't affect score output\n",
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every, step_reward, ghost_reward, policy_file_name, step_reward_multiplier):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in tqdm(range(1, n_training_episodes+1)):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state,game_env = env.reset()\n",
    "\n",
    "        step_num = 0\n",
    "        score_adjustments = 0\n",
    "        cur_step_reward = step_reward\n",
    "\n",
    "        for t in range(max_t):\n",
    "            old_game_env = game_env\n",
    "\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, game_env = env.step(action)\n",
    "            \n",
    "            # Equal reward for eating ghost\n",
    "            if reward // 100 == 2:\n",
    "              reward = reward - 200 + ghost_reward\n",
    "              score_adjustments += 200 - ghost_reward\n",
    "            elif reward // 100 == 4:\n",
    "              reward = reward - 400 + ghost_reward\n",
    "              score_adjustments += 400 - ghost_reward\n",
    "            elif reward // 100 == 8:\n",
    "              reward = reward - 800 + ghost_reward\n",
    "              score_adjustments += 800 - ghost_reward\n",
    "            elif reward // 100 == 16:\n",
    "              reward = reward - 1600 + ghost_reward\n",
    "              score_adjustments += 1600 - ghost_reward\n",
    "                \n",
    "            if reward % 100 == 10:\n",
    "                cur_step_reward = step_reward\n",
    "            else:\n",
    "                cur_step_reward *= step_reward_multiplier\n",
    "                \n",
    "            reward += step_reward\n",
    "            score_adjustments -= step_reward\n",
    "            \n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_sum = sum(rewards) + score_adjustments\n",
    "        scores_deque.append(reward_sum)\n",
    "        scores.append(reward_sum)\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = rewards[t] + gamma * (disc_return_t if t + 1 < n_steps else 0)\n",
    "          returns.appendleft(disc_return_t)\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    torch.save(policy, policy_file_name)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YUU61cWi0viw"
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"gamma\": 1e-2,\n",
    "    \"n_layers\": 2,\n",
    "    \"h_size\": 274,\n",
    "    \"dropout\": 0.4217,\n",
    "    \"lr\": 0.002333,\n",
    "    \"step_reward\": -1,\n",
    "    \"step_reward_multiplier\": 1.01,\n",
    "    \"ghost_reward\": -72,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"policy_file_name\": \"MsPacManPG_optimized.pt\",\n",
    "    \"n_training_episodes\": 100,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 5000,\n",
    "    \"env_id\": \"ALE/MsPacman-ram-v5\",\n",
    "    \"s_size\": 128,\n",
    "    \"a_size\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h3YigzWp0viw"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "# Create policy and place it to the device\n",
    "policy = Policy(hyperparameters[\"n_layers\"], hyperparameters[\"h_size\"],\n",
    "                hyperparameters[\"dropout\"], hyperparameters[\"s_size\"],\n",
    "                hyperparameters[\"a_size\"]).to(device)\n",
    "optimizer = getattr(optim, hyperparameters[\"optimizer\"])(policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oOrLawnv0viw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 10/100 [00:09<01:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 266.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▏                                                                | 20/100 [00:18<01:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 357.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▎                                                        | 30/100 [00:28<01:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30\tAverage Score: 386.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▍                                                | 40/100 [00:39<01:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40\tAverage Score: 419.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████▌                                        | 50/100 [00:52<01:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 416.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [01:04<00:41,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60\tAverage Score: 432.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████▋                        | 70/100 [01:16<00:40,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70\tAverage Score: 447.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████▊                | 80/100 [01:26<00:20,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80\tAverage Score: 447.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 90/100 [01:38<00:11,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90\tAverage Score: 449.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:49<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 450.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and save neural network\n",
    "scores = reinforce(policy,\n",
    "                   optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"],\n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   1 - hyperparameters[\"gamma\"],\n",
    "                   10, hyperparameters[\"step_reward\"],\n",
    "                   hyperparameters[\"ghost_reward\"],\n",
    "                   hyperparameters[\"policy_file_name\"], \n",
    "                   hyperparameters[\"step_reward_multiplier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hcMaUOkpX1VW"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, game_env = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, _, game_env = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFmmyptAsU6A"
   },
   "outputs": [],
   "source": [
    "# # Load saved policy\n",
    "# policy = torch.load(hyperparameters[\"policy_file_name\"])\n",
    "# policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IwvMTczpZ8G2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 506.0; Std: 266.39069052802876\n"
     ]
    }
   ],
   "source": [
    "mean, std = evaluate_agent(env, hyperparameters[\"max_t\"], 10, policy)\n",
    "print(f\"Mean: {mean}; Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "703fKAx4eM0Q"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  done = False\n",
    "  state, _ = env.reset()\n",
    "  score = 0\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    new_state, reward, done, _, game_env = env.step(action)\n",
    "    score += reward\n",
    "  print(score)\n",
    "  imageio.mimsave(out_directory, env.render(), fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4vZcv54ceR6k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480.0\n"
     ]
    }
   ],
   "source": [
    "replay_env = gym.make(\"ALE/MsPacman-ram-v5\", render_mode=\"rgb_array_list\")\n",
    "record_video(replay_env, policy, './MsPacMan_replay.mp4')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
