{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co5UkMLwJPgV"
   },
   "source": [
    "# Hyperparameter Optimizer\n",
    "References\n",
    "- _Optuna simple example_ https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py#L71  \n",
    "- _Optuna RL example_ https://github.com/optuna/optuna-examples/blob/main/rl/sb3_simple.py#L114\n",
    "- _Hugging Face policy gradient_ https://huggingface.co/learn/deep-rl-course/unit4/hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 100255,
     "status": "ok",
     "timestamp": 1701321733110,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "V9koMBy-IBjj"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install -U scikit-learn\n",
    "!pip install optuna\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install cmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9140,
     "status": "ok",
     "timestamp": 1701321973105,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "7VAeEd9QIB1c",
    "outputId": "7dda585a-0014-4c20-9145-8ed640e0f553"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Load environment\n",
    "env = gym.make(\"ALE/MsPacman-ram-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1701321973125,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "Vh_01QvmYuCn",
    "outputId": "97f851dc-7a7d-4337-e1e8-900249712ac9"
   },
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701321973134,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "e-RixhzX80QW"
   },
   "outputs": [],
   "source": [
    "default_hyperparams = {\n",
    "    \"epoch\": 100,  # max number of episodes per optimization trial\n",
    "#     \"n_training_episodes\": 5000,  TODO: Delete\n",
    "    \"max_t\": 5000,  # max number of steps per trial\n",
    "#     \"env_id\": \"ALE/MsPacman-ram-v5\",  TODO: Delete\n",
    "    \"state_space\": 128,  # RAM data for Atari console during game\n",
    "    \"action_space\": 5,  # No-op, up, right, left, down\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701321973142,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "8KYFai1g80TR"
   },
   "outputs": [],
   "source": [
    "# Based off Optuna RL example code\n",
    "# Changes by CS 175 project group: hyperparameters being sampled\n",
    "def sample_hyperparams(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for hyperparameters.\"\"\"\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.99, 1, log=True)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
    "    h_size = trial.suggest_int(\"h_size\", 4, 1024)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7, log=False)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    step_reward = trial.suggest_float(\"step_reward\", -10, 0, log=False)\n",
    "    step_reward_multiplier = trial.suggest_float(\"step_reward_multiplier\", 1, 1.1, log=True)\n",
    "    ghost_reward = trial.suggest_int(\"ghost_reward\", -1000, 1000)\n",
    "    # optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    optimizer = \"SGD\"\n",
    "\n",
    "    # Display true values.\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_layers_\", n_layers)\n",
    "    trial.set_user_attr(\"h_size_\", h_size)\n",
    "    trial.set_user_attr(\"dropout_\", dropout)\n",
    "    trial.set_user_attr(\"lr_\", lr)\n",
    "    trial.set_user_attr(\"optimizer_\", optimizer)\n",
    "    trial.set_user_attr(\"step_reward_\", step_reward)\n",
    "    trial.set_user_attr(\"step_reward_multiplier\", step_reward_multiplier)\n",
    "    trial.set_user_attr(\"ghost_reward_\", ghost_reward)\n",
    "\n",
    "    return {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"h_size\": h_size,\n",
    "        \"dropout\": dropout,\n",
    "        \"gamma\": gamma,\n",
    "        \"lr\": lr,\n",
    "        \"step_reward\": step_reward,\n",
    "        \"step_reward_multiplier\": step_reward_multiplier,\n",
    "        \"ghost_reward\": ghost_reward,\n",
    "        \"optimizer\": optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1701321973159,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "RL0VXxawQvKI"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group: \n",
    "#   - class inherits nn.Sequential rather than nn.Module\n",
    "#   - change to constructor method and deletion of explicitly defined forward method\n",
    "class Policy(nn.Sequential):\n",
    "  def __init__(self, n_layers, h_size, dropout, s_size, a_size):\n",
    "    layers = []\n",
    "\n",
    "    in_features = s_size\n",
    "    for i in range(n_layers):\n",
    "      layers.append(nn.Linear(in_features, h_size))\n",
    "      layers.append(nn.ReLU())\n",
    "      layers.append(nn.Dropout(dropout))\n",
    "\n",
    "      in_features = h_size\n",
    "    layers.append(nn.Linear(in_features, a_size))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    super().__init__(*layers)\n",
    "\n",
    "  def act(self, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs = self.forward(state).cpu()\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701321973167,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "cR2iUtPwUTBh"
   },
   "outputs": [],
   "source": [
    "# Contains policy trainer from Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group: \n",
    "#   - changes to reward for training\n",
    "#   - ensure changes to reward doesn't affect score output\n",
    "#   - added Optuna methods to evaluate episodes and prune trials if needed\n",
    "#   - cut out portions from original code not needed by trainer\n",
    "def train(trial, policy, optimizer, epoch, max_t, gamma, step_reward, ghost_reward, step_reward_multiplier):\n",
    "    for i_epoch in range(epoch):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state,game_env = env.reset()\n",
    "        \n",
    "        step_num = 0\n",
    "        score_adjustments = 0\n",
    "        cur_step_reward = step_reward\n",
    "\n",
    "        for t in range(max_t):\n",
    "            old_game_env = game_env\n",
    "\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, game_env = env.step(action)\n",
    "            \n",
    "            # Equal reward for eating ghost\n",
    "            if reward // 100 == 2:\n",
    "              reward = reward - 200 + ghost_reward\n",
    "              score_adjustments += 200 - ghost_reward\n",
    "            elif reward // 100 == 4:\n",
    "              reward = reward - 400 + ghost_reward\n",
    "              score_adjustments += 400 - ghost_reward\n",
    "            elif reward // 100 == 8:\n",
    "              reward = reward - 800 + ghost_reward\n",
    "              score_adjustments += 800 - ghost_reward\n",
    "            elif reward // 100 == 16:\n",
    "              reward = reward - 1600 + ghost_reward\n",
    "              score_adjustments += 1600 - ghost_reward\n",
    "                \n",
    "            if reward % 100 == 10:\n",
    "                cur_step_reward = step_reward\n",
    "            else:\n",
    "                cur_step_reward *= step_reward_multiplier\n",
    "                \n",
    "            reward += step_reward\n",
    "            score_adjustments -= step_reward\n",
    "            \n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        final_score = sum(rewards) + score_adjustments\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = rewards[t] + gamma * (disc_return_t if t + 1 < n_steps else 0)\n",
    "          returns.appendleft(disc_return_t)\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trial.report(final_score, i_epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701321831018,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "OY1rTCns80Y9"
   },
   "outputs": [],
   "source": [
    "# Based off Optuna simple example code\n",
    "# Changes by CS 175 project group: \n",
    "#   - replaced original policy with policy for Ms Pacman\n",
    "#   - consolidated training code into separate function (previous code box)\n",
    "def objective(trial):\n",
    "    hyperparameters = {**default_hyperparams, **sample_hyperparams(trial)}\n",
    "\n",
    "    # Generate the model.\n",
    "    policy = Policy(hyperparameters[\"n_layers\"], hyperparameters[\"h_size\"],\n",
    "                    hyperparameters[\"dropout\"], hyperparameters[\"state_space\"],\n",
    "                    hyperparameters[\"action_space\"]).to(device)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = hyperparameters[\"optimizer\"]\n",
    "    optimizer = getattr(optim, optimizer_name)(policy.parameters(), lr=hyperparameters[\"lr\"])\n",
    "\n",
    "    score = train(trial, policy, optimizer, hyperparameters[\"epoch\"],\n",
    "                  hyperparameters[\"max_t\"], hyperparameters[\"gamma\"],\n",
    "                  hyperparameters[\"step_reward\"], hyperparameters[\"ghost_reward\"],\n",
    "                  hyperparameters[\"step_reward_multiplier\"])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKJ4LDmf80bl",
    "outputId": "fee5cfce-7be8-4606-fe80-9f31cbbc2328"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:07:54,633] A new study created in RDB with name: MsPacMan_study_1201_1643\n"
     ]
    }
   ],
   "source": [
    "# Create an Optuna study\n",
    "# Study info will be saved at path given to \"storage\" parameter as .db file\n",
    "study = optuna.create_study(study_name=\"MsPacMan_study_1201_1643\", storage=\"sqlite:///MsPacMan_study_1201_1643.db\", \n",
    "                            direction=\"maximize\", \n",
    "                            # Recommend default sampler and pruner for <1000 trials\n",
    "                            # Comment out following two lines to use default sampler and pruner\n",
    "                            sampler=optuna.samplers.CmaEsSampler(consider_pruned_trials=False), \n",
    "#                             pruner=optuna.pruners.HyperbandPruner()\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved study\n",
    "# study = optuna.load_study(MsPacMan_study=\"test\", storage=\"sqlite:///MsPacMan_study.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:13:23,122] Trial 0 finished with value: 190.00000000000261 and parameters: {'gamma': 0.9963613921555072, 'n_layers': 10, 'h_size': 798, 'dropout': 0.23246628577821934, 'lr': 2.1691046430021075e-05, 'step_reward': -3.3678081050894146, 'step_reward_multiplier': 1.040410122340053, 'ghost_reward': -38}. Best is trial 0 with value: 190.00000000000261.\n",
      "[I 2023-12-01 19:13:23,749] Trial 1 finished with value: 49.99999999999986 and parameters: {'gamma': 0.993667637112442, 'n_layers': 10, 'h_size': 1020, 'dropout': 0.6944159830209767, 'lr': 0.00033806255918187233, 'step_reward': -0.1146722468481709, 'step_reward_multiplier': 1.0125097420611682, 'ghost_reward': -10}. Best is trial 0 with value: 190.00000000000261.\n",
      "[I 2023-12-01 19:13:25,115] Trial 3 finished with value: 59.99999999999966 and parameters: {'gamma': 0.9910252416242034, 'n_layers': 4, 'h_size': 518, 'dropout': 0.029878411801336446, 'lr': 0.03980724305105555, 'step_reward': -7.17671484634767, 'step_reward_multiplier': 1.0732844946136606, 'ghost_reward': -758}. Best is trial 0 with value: 190.00000000000261.\n",
      "[I 2023-12-01 19:13:25,215] Trial 4 finished with value: 99.9999999999992 and parameters: {'gamma': 0.998921263972811, 'n_layers': 1, 'h_size': 179, 'dropout': 0.5872160710109584, 'lr': 0.0003891045866174723, 'step_reward': -3.4975344504045136, 'step_reward_multiplier': 1.055094623854314, 'ghost_reward': -627}. Best is trial 0 with value: 190.00000000000261.\n",
      "[I 2023-12-01 19:13:25,315] Trial 2 finished with value: 69.99999999999955 and parameters: {'gamma': 0.9926050927056049, 'n_layers': 6, 'h_size': 730, 'dropout': 0.11672276548905915, 'lr': 0.03725358829945004, 'step_reward': -6.1351719032944, 'step_reward_multiplier': 1.0094924142240245, 'ghost_reward': -401}. Best is trial 0 with value: 190.00000000000261.\n",
      "[I 2023-12-01 19:13:28,855] Trial 7 pruned. \n",
      "[I 2023-12-01 19:13:29,426] Trial 8 pruned. \n",
      "[I 2023-12-01 19:14:05,052] Trial 10 pruned. \n",
      "[I 2023-12-01 19:14:07,659] Trial 12 pruned. \n",
      "[I 2023-12-01 19:14:28,784] Trial 13 pruned. \n",
      "[I 2023-12-01 19:14:31,855] Trial 14 pruned. \n",
      "[I 2023-12-01 19:14:52,124] Trial 15 pruned. \n",
      "[I 2023-12-01 19:14:55,533] Trial 16 pruned. \n",
      "[I 2023-12-01 19:14:58,827] Trial 17 pruned. \n",
      "[I 2023-12-01 19:15:02,253] Trial 11 pruned. \n",
      "[I 2023-12-01 19:15:05,746] Trial 19 pruned. \n",
      "[I 2023-12-01 19:15:09,287] Trial 20 pruned. \n",
      "[I 2023-12-01 19:15:13,135] Trial 21 pruned. \n",
      "[I 2023-12-01 19:15:16,845] Trial 22 pruned. \n",
      "[I 2023-12-01 19:15:20,442] Trial 23 pruned. \n",
      "[I 2023-12-01 19:15:40,976] Trial 18 pruned. \n",
      "[I 2023-12-01 19:15:41,647] Trial 24 pruned. \n",
      "[I 2023-12-01 19:15:48,750] Trial 26 pruned. \n",
      "[I 2023-12-01 19:15:52,645] Trial 27 pruned. \n",
      "[I 2023-12-01 19:15:59,479] Trial 28 pruned. \n",
      "[I 2023-12-01 19:16:03,501] Trial 29 pruned. \n",
      "[I 2023-12-01 19:16:07,712] Trial 30 pruned. \n",
      "[I 2023-12-01 19:16:30,543] Trial 31 pruned. \n",
      "[I 2023-12-01 19:16:37,140] Trial 32 pruned. \n",
      "[I 2023-12-01 19:18:38,635] Trial 5 finished with value: 709.99999999997 and parameters: {'gamma': 0.9953493534827103, 'n_layers': 4, 'h_size': 330, 'dropout': 0.2433837525159233, 'lr': 0.00020626544245803326, 'step_reward': -4.704061210139283, 'step_reward_multiplier': 1.0367194943117792, 'ghost_reward': -43}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:18:39,237] Trial 6 finished with value: 150.0 and parameters: {'gamma': 0.9954733984639477, 'n_layers': 5, 'h_size': 288, 'dropout': 0.3375180017979989, 'lr': 0.007653159959817036, 'step_reward': -5.297017462195994, 'step_reward_multiplier': 1.0336735766316967, 'ghost_reward': 398}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:18:39,892] Trial 9 finished with value: 699.9999999999982 and parameters: {'gamma': 0.9945749173504373, 'n_layers': 4, 'h_size': 430, 'dropout': 0.37125387678355043, 'lr': 0.0002151917912535441, 'step_reward': -3.105025219078513, 'step_reward_multiplier': 1.0444766324955086, 'ghost_reward': -517}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:18:46,210] Trial 36 pruned. \n",
      "[I 2023-12-01 19:21:17,936] Trial 25 finished with value: 120.0 and parameters: {'gamma': 0.9948880117131513, 'n_layers': 5, 'h_size': 559, 'dropout': 0.24991159816153424, 'lr': 0.0006421360454673274, 'step_reward': -4.445565692689922, 'step_reward_multiplier': 1.064367997211373, 'ghost_reward': -277}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:21:44,172] Trial 38 pruned. \n",
      "[I 2023-12-01 19:21:51,496] Trial 39 pruned. \n",
      "[I 2023-12-01 19:21:55,073] Trial 40 pruned. \n",
      "[I 2023-12-01 19:21:58,647] Trial 41 pruned. \n",
      "[I 2023-12-01 19:22:16,588] Trial 33 finished with value: 40.0 and parameters: {'gamma': 0.9954032311331165, 'n_layers': 4, 'h_size': 621, 'dropout': 0.22246777896327719, 'lr': 0.0005202077803029363, 'step_reward': -5.114645253526185, 'step_reward_multiplier': 1.0590532646617419, 'ghost_reward': -35}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:22:24,574] Trial 42 pruned. \n",
      "[I 2023-12-01 19:22:25,214] Trial 43 pruned. \n",
      "[I 2023-12-01 19:22:33,574] Trial 44 pruned. \n",
      "[I 2023-12-01 19:22:34,291] Trial 45 pruned. \n",
      "[I 2023-12-01 19:22:41,882] Trial 47 pruned. \n",
      "[I 2023-12-01 19:22:46,800] Trial 48 pruned. \n",
      "[I 2023-12-01 19:23:14,195] Trial 49 pruned. \n",
      "[I 2023-12-01 19:23:18,319] Trial 50 pruned. \n",
      "[I 2023-12-01 19:23:49,992] Trial 51 pruned. \n",
      "[I 2023-12-01 19:23:57,613] Trial 52 pruned. \n",
      "[I 2023-12-01 19:24:01,286] Trial 53 pruned. \n",
      "[I 2023-12-01 19:24:09,277] Trial 54 pruned. \n",
      "[I 2023-12-01 19:24:35,802] Trial 55 pruned. \n",
      "[I 2023-12-01 19:24:44,010] Trial 56 pruned. \n",
      "[I 2023-12-01 19:24:53,065] Trial 57 pruned. \n",
      "[I 2023-12-01 19:24:55,728] Trial 34 finished with value: 430.00000000000364 and parameters: {'gamma': 0.9949345207313701, 'n_layers': 6, 'h_size': 728, 'dropout': 0.5383778314916969, 'lr': 0.0015635309737979386, 'step_reward': -3.9060751726446448, 'step_reward_multiplier': 1.0738914935575767, 'ghost_reward': -401}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:24:56,545] Trial 35 finished with value: 140.0000000000016 and parameters: {'gamma': 0.9955215702753439, 'n_layers': 7, 'h_size': 605, 'dropout': 0.4426905728839923, 'lr': 0.0010290175200322216, 'step_reward': -3.6821052245360875, 'step_reward_multiplier': 1.0565871767275172, 'ghost_reward': -332}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:24:58,250] Trial 58 pruned. \n",
      "[I 2023-12-01 19:25:02,662] Trial 61 pruned. \n",
      "[I 2023-12-01 19:25:06,372] Trial 37 finished with value: 149.99999999999977 and parameters: {'gamma': 0.9970309263136666, 'n_layers': 6, 'h_size': 461, 'dropout': 0.23166304830365975, 'lr': 6.45609903097798e-05, 'step_reward': -3.6228843310159586, 'step_reward_multiplier': 1.0181695095957433, 'ghost_reward': -247}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:25:07,174] Trial 62 pruned. \n",
      "[I 2023-12-01 19:25:11,339] Trial 64 pruned. \n",
      "[I 2023-12-01 19:25:15,851] Trial 65 pruned. \n",
      "[I 2023-12-01 19:25:19,894] Trial 66 pruned. \n",
      "[I 2023-12-01 19:25:28,032] Trial 67 pruned. \n",
      "[I 2023-12-01 19:25:32,228] Trial 68 pruned. \n",
      "[I 2023-12-01 19:25:36,277] Trial 69 pruned. \n",
      "[I 2023-12-01 19:25:40,335] Trial 63 pruned. \n",
      "[I 2023-12-01 19:25:44,700] Trial 70 pruned. \n",
      "[I 2023-12-01 19:25:48,526] Trial 72 pruned. \n",
      "[I 2023-12-01 19:25:52,784] Trial 73 pruned. \n",
      "[I 2023-12-01 19:25:57,982] Trial 74 pruned. \n",
      "[I 2023-12-01 19:26:08,011] Trial 75 pruned. \n",
      "[I 2023-12-01 19:26:13,008] Trial 76 pruned. \n",
      "[I 2023-12-01 19:26:18,191] Trial 77 pruned. \n",
      "[I 2023-12-01 19:26:23,136] Trial 78 pruned. \n",
      "[I 2023-12-01 19:26:27,764] Trial 79 pruned. \n",
      "[I 2023-12-01 19:26:31,823] Trial 80 pruned. \n",
      "[I 2023-12-01 19:26:35,802] Trial 81 pruned. \n",
      "[I 2023-12-01 19:26:40,298] Trial 82 pruned. \n",
      "[I 2023-12-01 19:26:44,879] Trial 83 pruned. \n",
      "[I 2023-12-01 19:26:59,637] Trial 84 pruned. \n",
      "[I 2023-12-01 19:27:04,018] Trial 85 pruned. \n",
      "[I 2023-12-01 19:27:07,975] Trial 86 pruned. \n",
      "[I 2023-12-01 19:27:12,318] Trial 87 pruned. \n",
      "[I 2023-12-01 19:27:18,118] Trial 88 pruned. \n",
      "[I 2023-12-01 19:27:23,771] Trial 89 pruned. \n",
      "[I 2023-12-01 19:27:38,268] Trial 90 pruned. \n",
      "[I 2023-12-01 19:27:41,941] Trial 91 pruned. \n",
      "[I 2023-12-01 19:27:47,108] Trial 92 pruned. \n",
      "[I 2023-12-01 19:27:51,340] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:27:55,937] Trial 94 pruned. \n",
      "[I 2023-12-01 19:28:00,331] Trial 95 pruned. \n",
      "[I 2023-12-01 19:28:04,772] Trial 96 pruned. \n",
      "[I 2023-12-01 19:28:09,413] Trial 97 pruned. \n",
      "[I 2023-12-01 19:28:17,605] Trial 98 pruned. \n",
      "[I 2023-12-01 19:28:21,471] Trial 99 pruned. \n",
      "[I 2023-12-01 19:28:29,683] Trial 100 pruned. \n",
      "[I 2023-12-01 19:28:44,542] Trial 101 pruned. \n",
      "[I 2023-12-01 19:28:48,089] Trial 102 pruned. \n",
      "[I 2023-12-01 19:28:52,147] Trial 103 pruned. \n",
      "[I 2023-12-01 19:28:56,760] Trial 104 pruned. \n",
      "[I 2023-12-01 19:29:01,126] Trial 105 pruned. \n",
      "[I 2023-12-01 19:29:05,267] Trial 106 pruned. \n",
      "[I 2023-12-01 19:29:09,431] Trial 107 pruned. \n",
      "[I 2023-12-01 19:29:13,504] Trial 108 pruned. \n",
      "[I 2023-12-01 19:29:18,302] Trial 109 pruned. \n",
      "[I 2023-12-01 19:29:26,876] Trial 110 pruned. \n",
      "[I 2023-12-01 19:29:31,125] Trial 111 pruned. \n",
      "[I 2023-12-01 19:29:35,214] Trial 46 finished with value: 39.999999999999545 and parameters: {'gamma': 0.9949977093602713, 'n_layers': 4, 'h_size': 466, 'dropout': 0.45659726039600884, 'lr': 0.00021173355272535932, 'step_reward': -6.870668408922106, 'step_reward_multiplier': 1.0182340857670407, 'ghost_reward': 387}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:29:39,768] Trial 112 pruned. \n",
      "[I 2023-12-01 19:29:43,663] Trial 114 pruned. \n",
      "[I 2023-12-01 19:29:58,012] Trial 115 pruned. \n",
      "[I 2023-12-01 19:30:00,889] Trial 116 pruned. \n",
      "[I 2023-12-01 19:30:04,631] Trial 117 pruned. \n",
      "[I 2023-12-01 19:30:08,464] Trial 118 pruned. \n",
      "[I 2023-12-01 19:30:11,981] Trial 119 pruned. \n",
      "[I 2023-12-01 19:30:15,908] Trial 120 pruned. \n",
      "[I 2023-12-01 19:30:19,797] Trial 121 pruned. \n",
      "[I 2023-12-01 19:30:36,183] Trial 122 pruned. \n",
      "[I 2023-12-01 19:30:50,215] Trial 123 pruned. \n",
      "[I 2023-12-01 19:31:25,499] Trial 59 finished with value: 160.00000000000068 and parameters: {'gamma': 0.9956765334784446, 'n_layers': 6, 'h_size': 692, 'dropout': 0.3618297825692323, 'lr': 0.013943791576338835, 'step_reward': -5.315636334007148, 'step_reward_multiplier': 1.0148792849143804, 'ghost_reward': -2}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:31:26,085] Trial 60 finished with value: 80.0 and parameters: {'gamma': 0.9971642252276018, 'n_layers': 9, 'h_size': 197, 'dropout': 0.27983633026802773, 'lr': 0.009099034396666485, 'step_reward': -2.628689280524732, 'step_reward_multiplier': 1.0787981766417714, 'ghost_reward': 470}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:31:51,911] Trial 71 finished with value: 49.999999999999886 and parameters: {'gamma': 0.9976054336241676, 'n_layers': 5, 'h_size': 391, 'dropout': 0.35530447500845613, 'lr': 0.0005436903227232863, 'step_reward': -6.247406020502725, 'step_reward_multiplier': 1.0614865040017316, 'ghost_reward': 358}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:31:54,511] Trial 127 pruned. \n",
      "[I 2023-12-01 19:31:57,033] Trial 128 pruned. \n",
      "[I 2023-12-01 19:31:59,578] Trial 129 pruned. \n",
      "[I 2023-12-01 19:32:02,401] Trial 130 pruned. \n",
      "[I 2023-12-01 19:32:04,775] Trial 131 pruned. \n",
      "[I 2023-12-01 19:32:07,244] Trial 132 pruned. \n",
      "[I 2023-12-01 19:32:09,535] Trial 133 pruned. \n",
      "[I 2023-12-01 19:32:13,900] Trial 134 pruned. \n",
      "[I 2023-12-01 19:32:16,153] Trial 135 pruned. \n",
      "[I 2023-12-01 19:32:19,531] Trial 136 pruned. \n",
      "[I 2023-12-01 19:32:22,370] Trial 137 pruned. \n",
      "[I 2023-12-01 19:32:24,924] Trial 138 pruned. \n",
      "[I 2023-12-01 19:32:27,886] Trial 139 pruned. \n",
      "[I 2023-12-01 19:32:30,893] Trial 140 pruned. \n",
      "[I 2023-12-01 19:32:34,408] Trial 141 pruned. \n",
      "[I 2023-12-01 19:32:37,201] Trial 142 pruned. \n",
      "[I 2023-12-01 19:32:39,792] Trial 143 pruned. \n",
      "[I 2023-12-01 19:32:44,236] Trial 144 pruned. \n",
      "[I 2023-12-01 19:32:47,006] Trial 145 pruned. \n",
      "[I 2023-12-01 19:32:52,928] Trial 146 pruned. \n",
      "[I 2023-12-01 19:32:55,868] Trial 147 pruned. \n",
      "[I 2023-12-01 19:33:00,465] Trial 148 pruned. \n",
      "[I 2023-12-01 19:33:02,934] Trial 149 pruned. \n",
      "[I 2023-12-01 19:33:05,340] Trial 150 pruned. \n",
      "[I 2023-12-01 19:33:07,769] Trial 151 pruned. \n",
      "[I 2023-12-01 19:33:09,906] Trial 152 pruned. \n",
      "[I 2023-12-01 19:33:14,418] Trial 153 pruned. \n",
      "[I 2023-12-01 19:33:17,927] Trial 154 pruned. \n",
      "[I 2023-12-01 19:33:20,804] Trial 155 pruned. \n",
      "[I 2023-12-01 19:33:24,075] Trial 156 pruned. \n",
      "[I 2023-12-01 19:33:26,883] Trial 157 pruned. \n",
      "[I 2023-12-01 19:33:29,373] Trial 158 pruned. \n",
      "[I 2023-12-01 19:33:32,867] Trial 159 pruned. \n",
      "[I 2023-12-01 19:33:36,075] Trial 160 pruned. \n",
      "[I 2023-12-01 19:33:38,952] Trial 161 pruned. \n",
      "[I 2023-12-01 19:33:41,691] Trial 162 pruned. \n",
      "[I 2023-12-01 19:33:44,244] Trial 163 pruned. \n",
      "[I 2023-12-01 19:33:46,573] Trial 164 pruned. \n",
      "[I 2023-12-01 19:33:48,957] Trial 165 pruned. \n",
      "[I 2023-12-01 19:33:51,148] Trial 166 pruned. \n",
      "[I 2023-12-01 19:33:53,755] Trial 167 pruned. \n",
      "[I 2023-12-01 19:33:56,578] Trial 168 pruned. \n",
      "[I 2023-12-01 19:34:00,025] Trial 169 pruned. \n",
      "[I 2023-12-01 19:34:11,238] Trial 170 pruned. \n",
      "[I 2023-12-01 19:34:13,867] Trial 171 pruned. \n",
      "[I 2023-12-01 19:34:16,475] Trial 172 pruned. \n",
      "[I 2023-12-01 19:34:18,928] Trial 173 pruned. \n",
      "[I 2023-12-01 19:34:19,683] Trial 113 finished with value: 40.000000000000114 and parameters: {'gamma': 0.9930597967944967, 'n_layers': 1, 'h_size': 709, 'dropout': 0.5108416518769214, 'lr': 0.00026660386913260163, 'step_reward': -7.1722815182859865, 'step_reward_multiplier': 1.0671788773388713, 'ghost_reward': -656}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:34:23,353] Trial 175 pruned. \n",
      "[I 2023-12-01 19:34:26,863] Trial 176 pruned. \n",
      "[I 2023-12-01 19:34:30,050] Trial 177 pruned. \n",
      "[I 2023-12-01 19:34:33,629] Trial 178 pruned. \n",
      "[I 2023-12-01 19:34:37,408] Trial 179 pruned. \n",
      "[I 2023-12-01 19:34:40,938] Trial 180 pruned. \n",
      "[I 2023-12-01 19:34:56,033] Trial 181 pruned. \n",
      "[I 2023-12-01 19:34:59,775] Trial 182 pruned. \n",
      "[I 2023-12-01 19:35:03,424] Trial 183 pruned. \n",
      "[I 2023-12-01 19:35:07,320] Trial 184 pruned. \n",
      "[I 2023-12-01 19:35:14,846] Trial 185 pruned. \n",
      "[I 2023-12-01 19:35:18,487] Trial 186 pruned. \n",
      "[I 2023-12-01 19:35:21,981] Trial 187 pruned. \n",
      "[I 2023-12-01 19:35:25,244] Trial 188 pruned. \n",
      "[I 2023-12-01 19:35:28,615] Trial 189 pruned. \n",
      "[I 2023-12-01 19:35:32,194] Trial 190 pruned. \n",
      "[I 2023-12-01 19:35:35,457] Trial 191 pruned. \n",
      "[I 2023-12-01 19:35:38,443] Trial 192 pruned. \n",
      "[I 2023-12-01 19:35:41,790] Trial 193 pruned. \n",
      "[I 2023-12-01 19:35:42,677] Trial 124 finished with value: 49.999999999999886 and parameters: {'gamma': 0.9909795245658467, 'n_layers': 3, 'h_size': 501, 'dropout': 0.5198201336095061, 'lr': 0.0006615901849910846, 'step_reward': -8.461432953322102, 'step_reward_multiplier': 1.0527702426525667, 'ghost_reward': 699}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:35:54,949] Trial 194 pruned. \n",
      "[I 2023-12-01 19:36:20,069] Trial 125 finished with value: 400.0000000000018 and parameters: {'gamma': 0.9950104286114093, 'n_layers': 4, 'h_size': 446, 'dropout': 0.26034409093923344, 'lr': 0.0009399054899670941, 'step_reward': -5.5284374667907805, 'step_reward_multiplier': 1.0442454254645894, 'ghost_reward': -394}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:36:20,702] Trial 126 finished with value: 110.00000000000057 and parameters: {'gamma': 0.9925215881412439, 'n_layers': 7, 'h_size': 496, 'dropout': 0.23185342520687094, 'lr': 0.0006918077244769995, 'step_reward': -3.679431688837865, 'step_reward_multiplier': 1.0138659465275484, 'ghost_reward': -175}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:40:03,609] Trial 174 finished with value: 120.00000000000045 and parameters: {'gamma': 0.9964026202867342, 'n_layers': 4, 'h_size': 354, 'dropout': 0.34035759862142095, 'lr': 0.0016063828039979463, 'step_reward': -5.311112463459677, 'step_reward_multiplier': 1.040424063432979, 'ghost_reward': -103}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:40:06,849] Trial 199 pruned. \n",
      "[I 2023-12-01 19:41:42,750] Trial 195 finished with value: 20.0 and parameters: {'gamma': 0.9969938084759675, 'n_layers': 7, 'h_size': 461, 'dropout': 0.5829614933587555, 'lr': 0.0005987746738189552, 'step_reward': -7.2115250858643325, 'step_reward_multiplier': 1.0427617187976448, 'ghost_reward': -690}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:41:46,421] Trial 201 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-01 19:41:50,005] Trial 196 finished with value: 50.00000000000034 and parameters: {'gamma': 0.9973897694026614, 'n_layers': 5, 'h_size': 829, 'dropout': 0.3687570309245692, 'lr': 0.004454036644410375, 'step_reward': -6.019554824854563, 'step_reward_multiplier': 1.0556393912948214, 'ghost_reward': -519}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:41:50,664] Trial 202 pruned. \n",
      "[I 2023-12-01 19:41:53,942] Trial 203 pruned. \n",
      "[I 2023-12-01 19:41:54,702] Trial 204 pruned. \n",
      "[I 2023-12-01 19:41:58,587] Trial 205 pruned. \n",
      "[I 2023-12-01 19:41:59,253] Trial 206 pruned. \n",
      "[I 2023-12-01 19:42:02,976] Trial 207 pruned. \n",
      "[I 2023-12-01 19:42:03,722] Trial 208 pruned. \n",
      "[I 2023-12-01 19:42:07,244] Trial 209 pruned. \n",
      "[I 2023-12-01 19:42:08,042] Trial 210 pruned. \n",
      "[I 2023-12-01 19:42:12,421] Trial 212 pruned. \n",
      "[I 2023-12-01 19:42:14,617] Trial 197 finished with value: 230.0 and parameters: {'gamma': 0.996578809080259, 'n_layers': 5, 'h_size': 435, 'dropout': 0.4474248690076819, 'lr': 0.0012765136745759957, 'step_reward': -5.103248142403587, 'step_reward_multiplier': 1.0372641600521317, 'ghost_reward': 273}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:42:15,227] Trial 198 finished with value: 240.0 and parameters: {'gamma': 0.9941495838675686, 'n_layers': 3, 'h_size': 617, 'dropout': 0.5083143393426369, 'lr': 0.00027034844420733325, 'step_reward': -4.323919754973985, 'step_reward_multiplier': 1.0455331564014152, 'ghost_reward': -322}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:42:16,295] Trial 213 pruned. \n",
      "[I 2023-12-01 19:42:18,710] Trial 214 pruned. \n",
      "[I 2023-12-01 19:42:20,138] Trial 216 pruned. \n",
      "[I 2023-12-01 19:42:21,337] Trial 217 pruned. \n",
      "[I 2023-12-01 19:42:23,458] Trial 219 pruned. \n",
      "[I 2023-12-01 19:42:26,832] Trial 220 pruned. \n",
      "[I 2023-12-01 19:45:36,785] Trial 200 finished with value: 120.00000000000102 and parameters: {'gamma': 0.9950847868485105, 'n_layers': 3, 'h_size': 398, 'dropout': 0.22622380791156588, 'lr': 4.183734643494094e-05, 'step_reward': -4.289938143276872, 'step_reward_multiplier': 1.044753357597559, 'ghost_reward': -87}. Best is trial 5 with value: 709.99999999997.\n",
      "[I 2023-12-01 19:45:40,849] Trial 222 pruned. \n"
     ]
    }
   ],
   "source": [
    "# Start Optuna study\n",
    "# show_progress_bar=True wouldn't work on Jupyter Notebook without installing Google Colab package\n",
    "# n_jobs: number of parallel jobs\n",
    "study.optimize(objective, n_trials=None, timeout=3600, n_jobs=5, gc_after_trial=True, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWq2Gs3ApGSd"
   },
   "outputs": [],
   "source": [
    "# Recommended hyperparameters from Optuna study\n",
    "# Exact code from Optuna simple example\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value:  \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1400,
     "status": "ok",
     "timestamp": 1701303248827,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "69P20lEW80eP",
    "outputId": "350da55c-62d1-4c34-e293-0c098bb007c1"
   },
   "outputs": [],
   "source": [
    "# Importance evaluation for each hyperparameter from Optuna study\n",
    "print(\"Importances:\")\n",
    "for key, value in optuna.importance.get_param_importances(study).items():\n",
    "  print(key, \":\", value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOHoAH5rAQtqqEbSTdHiU55",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
