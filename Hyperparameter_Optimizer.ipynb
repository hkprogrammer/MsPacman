{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co5UkMLwJPgV"
   },
   "source": [
    "# Hyperparameter Optimizer\n",
    "References\n",
    "- _Optuna simple example_ https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py#L71  \n",
    "- _Optuna RL example_ https://github.com/optuna/optuna-examples/blob/main/rl/sb3_simple.py#L114\n",
    "- _Hugging Face policy gradient_ https://huggingface.co/learn/deep-rl-course/unit4/hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 100255,
     "status": "ok",
     "timestamp": 1701321733110,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "V9koMBy-IBjj"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install -U scikit-learn\n",
    "!pip install optuna\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install cmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9140,
     "status": "ok",
     "timestamp": 1701321973105,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "7VAeEd9QIB1c",
    "outputId": "7dda585a-0014-4c20-9145-8ed640e0f553"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Load environment\n",
    "env = gym.make(\"ALE/MsPacman-ram-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1701321973125,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "Vh_01QvmYuCn",
    "outputId": "97f851dc-7a7d-4337-e1e8-900249712ac9"
   },
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701321973134,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "e-RixhzX80QW"
   },
   "outputs": [],
   "source": [
    "default_hyperparams = {\n",
    "    \"epoch\": 20,  # max number of episodes per optimization trial\n",
    "#     \"n_training_episodes\": 5000,  TODO: Delete\n",
    "    \"max_t\": 50000,  # max number of steps per trial\n",
    "#     \"env_id\": \"ALE/MsPacman-ram-v5\",  TODO: Delete\n",
    "    \"state_space\": 128,  # RAM data for Atari console during game\n",
    "    \"action_space\": 5,  # No-op, up, right, left, down\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701321973142,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "8KYFai1g80TR"
   },
   "outputs": [],
   "source": [
    "# Based off Optuna RL example code\n",
    "# Changes by CS 175 project group: hyperparameters being sampled\n",
    "def sample_hyperparams(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for hyperparameters.\"\"\"\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.99995, 1, log=True)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    h_size = trial.suggest_int(\"h_size\", 4, 1024)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7, log=False)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-2, log=True)\n",
    "    longevity_exponential = trial.suggest_float(\"longevity_exponential\", 1.001, 1.01, log=True)\n",
    "    step_penalty_multiplier = trial.suggest_float(\"step_penalty_multiplier\", 1, 1.1, log=True)\n",
    "#     ghost_reward = trial.suggest_int(\"ghost_reward\", -1000, 1000)\n",
    "    ghost_reward = 0\n",
    "    dot_extra_reward = trial.suggest_int(\"dot_extra_reward\", 0, 20)\n",
    "    energy_pill_extra_reward = trial.suggest_int(\"energy_pill_extra_reward\", 0, 100)\n",
    "    # optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    optimizer = \"SGD\"\n",
    "\n",
    "    # Display true values.\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_layers_\", n_layers)\n",
    "    trial.set_user_attr(\"h_size_\", h_size)\n",
    "    trial.set_user_attr(\"dropout_\", dropout)\n",
    "    trial.set_user_attr(\"lr_\", lr)\n",
    "    trial.set_user_attr(\"longevity_exponential_\", longevity_exponential)\n",
    "    trial.set_user_attr(\"step_penalty_multiplier_\", step_penalty_multiplier)\n",
    "    trial.set_user_attr(\"ghost_reward_\", ghost_reward)\n",
    "    trial.set_user_attr(\"dot_extra_reward_\", dot_extra_reward)\n",
    "    trial.set_user_attr(\"energy_pill_extra_reward_\", energy_pill_extra_reward)\n",
    "    trial.set_user_attr(\"optimizer_\", optimizer)\n",
    "\n",
    "    return {\n",
    "        \"gamma\": gamma,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"h_size\": h_size,\n",
    "        \"dropout\": dropout,\n",
    "        \"lr\": lr,\n",
    "        \"longevity_exponential\": longevity_exponential,\n",
    "        \"step_penalty_multiplier\": step_penalty_multiplier,\n",
    "        \"ghost_reward\": ghost_reward,\n",
    "        \"dot_extra_reward\": dot_extra_reward,\n",
    "        \"energy_pill_extra_reward\": energy_pill_extra_reward,\n",
    "        \"optimizer\": optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1701321973159,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "RL0VXxawQvKI"
   },
   "outputs": [],
   "source": [
    "# Based off Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group: \n",
    "#   - class inherits nn.Sequential rather than nn.Module\n",
    "#   - change to constructor method and deletion of explicitly defined forward method\n",
    "class Policy(nn.Sequential):\n",
    "  def __init__(self, n_layers, h_size, dropout, s_size, a_size):\n",
    "    layers = []\n",
    "\n",
    "    in_features = s_size\n",
    "    for i in range(n_layers):\n",
    "      layers.append(nn.Linear(in_features, h_size))\n",
    "      layers.append(nn.ReLU())\n",
    "      layers.append(nn.Dropout(dropout))\n",
    "\n",
    "      in_features = h_size\n",
    "    layers.append(nn.Linear(in_features, a_size))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    super().__init__(*layers)\n",
    "\n",
    "  def act(self, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs = self.forward(state).cpu()\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701321973167,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "cR2iUtPwUTBh"
   },
   "outputs": [],
   "source": [
    "# Contains policy trainer from Hugging Face policy gradient code\n",
    "# Changes by CS 175 project group: \n",
    "#   - changes to reward for training\n",
    "#   - ensure changes to reward doesn't affect score output\n",
    "#   - added Optuna methods to evaluate episodes and prune trials if needed\n",
    "#   - cut out portions from original code not needed by trainer\n",
    "def train(trial, policy, optimizer, epoch, max_t, gamma, ghost_reward, step_penalty_multiplier, \n",
    "          longevity_exponential=0, dot_extra_reward=0, energy_pill_extra_reward=0):\n",
    "    for i_epoch in range(epoch + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state,game_env = env.reset()\n",
    "        \n",
    "        # Variables for reward changes\n",
    "        step_num = 0\n",
    "        score_adjustments = 0\n",
    "        rewards_this_life = 0\n",
    "        step_penalty = 1\n",
    "        cur_step_penalty = step_penalty\n",
    "\n",
    "        for t in range(max_t):\n",
    "            old_game_env = game_env\n",
    "\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, game_env = env.step(action)\n",
    "            \n",
    "            # Longevity reward. More reward gathered for each life, larger reward\n",
    "            if old_game_env[\"lives\"] > game_env[\"lives\"]:\n",
    "                longevity_reward = longevity_exponential ** rewards_this_life\n",
    "                rewards_this_life = 0\n",
    "                reward += longevity_reward\n",
    "                score_adjustments -= longevity_reward\n",
    "                rewards.append(reward)\n",
    "                continue\n",
    "                \n",
    "            reward_change = 0\n",
    "            \n",
    "            # Equal penalty for eating ghost\n",
    "            if reward // 100 == 2:\n",
    "              reward_change = reward - 200 + ghost_reward\n",
    "              score_adjustments += 200 - ghost_reward\n",
    "            elif reward // 100 == 4:\n",
    "              reward_change = reward - 400 + ghost_reward\n",
    "              score_adjustments += 400 - ghost_reward\n",
    "            elif reward // 100 == 8:\n",
    "              reward_change = reward - 800 + ghost_reward\n",
    "              score_adjustments += 800 - ghost_reward\n",
    "            elif reward // 100 == 16:\n",
    "              reward_change = reward - 1600 + ghost_reward\n",
    "              score_adjustments += 1600 - ghost_reward\n",
    "                \n",
    "            # Penalty for going many steps without eating dot\n",
    "            if reward % 100 == 10:\n",
    "                cur_step_penalty = step_penalty\n",
    "                reward_change += dot_extra_reward\n",
    "                score_adjustments -= dot_extra_reward\n",
    "            elif reward % 100 == 50:\n",
    "                cur_step_penalty = step_penalty\n",
    "                reward_change += energy_pill_extra_reward\n",
    "                score_adjustments -= energy_pill_extra_reward\n",
    "            else:\n",
    "                cur_step_penalty *= step_penalty_multiplier\n",
    "                reward_change -= step_penalty\n",
    "                score_adjustments += step_penalty\n",
    "            \n",
    "            rewards.append(reward + reward_change)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        final_score = sum(rewards) + score_adjustments\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "\n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = rewards[t] + gamma * (disc_return_t if t + 1 < n_steps else 0)\n",
    "          returns.appendleft(disc_return_t)\n",
    "\n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trial.report(final_score, i_epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701321831018,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "OY1rTCns80Y9"
   },
   "outputs": [],
   "source": [
    "# Based off Optuna simple example code\n",
    "# Changes by CS 175 project group: \n",
    "#   - replaced original policy with policy for Ms Pacman\n",
    "#   - consolidated training code into separate function (previous code box)\n",
    "def objective(trial):\n",
    "    hyperparameters = {**default_hyperparams, **sample_hyperparams(trial)}\n",
    "\n",
    "    # Generate the model.\n",
    "    policy = Policy(hyperparameters[\"n_layers\"], hyperparameters[\"h_size\"],\n",
    "                    hyperparameters[\"dropout\"], hyperparameters[\"state_space\"],\n",
    "                    hyperparameters[\"action_space\"]).to(device)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = hyperparameters[\"optimizer\"]\n",
    "    optimizer = getattr(optim, optimizer_name)(policy.parameters(), lr=hyperparameters[\"lr\"])\n",
    "\n",
    "    score = train(trial, policy, optimizer, epoch=hyperparameters[\"epoch\"],\n",
    "                  max_t=hyperparameters[\"max_t\"], gamma=hyperparameters[\"gamma\"],\n",
    "                  ghost_reward=hyperparameters[\"ghost_reward\"],\n",
    "                  step_penalty_multiplier=hyperparameters[\"step_penalty_multiplier\"],\n",
    "                  longevity_exponential=hyperparameters[\"longevity_exponential\"],\n",
    "                  dot_extra_reward=hyperparameters[\"dot_extra_reward\"],\n",
    "                  energy_pill_extra_reward=hyperparameters[\"energy_pill_extra_reward\"],\n",
    "                 )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKJ4LDmf80bl",
    "outputId": "fee5cfce-7be8-4606-fe80-9f31cbbc2328"
   },
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "# Study info will be saved at path given to \"storage\" parameter as .db file\n",
    "study = optuna.create_study(study_name=\"MsPacMan_study\", storage=\"sqlite:///MsPacMan_study.db\", \n",
    "                            direction=\"maximize\", \n",
    "                            # Recommend default sampler and pruner for <1000 trials\n",
    "                            # Comment out following two lines to use default sampler and pruner\n",
    "                            sampler=optuna.samplers.CmaEsSampler(consider_pruned_trials=False), \n",
    "#                             pruner=optuna.pruners.HyperbandPruner()\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved study\n",
    "study = optuna.load_study(study_name=\"MsPacMan_study\", storage=\"sqlite:///Studies/MsPacMan_study.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Optuna study\n",
    "# show_progress_bar=True wouldn't work on Jupyter Notebook without installing Google Colab package\n",
    "# n_jobs: number of parallel jobs\n",
    "study.optimize(objective, n_trials=None, timeout=None, n_jobs=5, gc_after_trial=True, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWq2Gs3ApGSd"
   },
   "outputs": [],
   "source": [
    "# Recommended hyperparameters from Optuna study\n",
    "# Exact code from Optuna simple example\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value:  \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1400,
     "status": "ok",
     "timestamp": 1701303248827,
     "user": {
      "displayName": "Jordan Jin",
      "userId": "09952125326305010607"
     },
     "user_tz": 480
    },
    "id": "69P20lEW80eP",
    "outputId": "350da55c-62d1-4c34-e293-0c098bb007c1"
   },
   "outputs": [],
   "source": [
    "# Importance evaluation for each hyperparameter from Optuna study\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "\n",
    "# print(\"Importances:\")\n",
    "# for key, value in optuna.importance.get_param_importances(study).items():\n",
    "#   print(key, \":\", value)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOHoAH5rAQtqqEbSTdHiU55",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
